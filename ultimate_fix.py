import os

target_file = "backend/scripts/crawler.py"

crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin, quote
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, headless=True):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Referer': 'https://m.ebcblue.com/'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        st.info(f"ğŸŒ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì •ë°€ ìŠ¤ìº” ì¤‘: {url}")
        raw_links = self.get_post_links(url, keyword)
        return {'notice': [], 'normal': raw_links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            # 1. SSL ì¸ì¦ì„œ ê²€ì‚¬ ë¬´ì‹œí•˜ê³  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # 2. ê²Œì‹œíŒì˜ ëª¨ë“  'ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ' ë˜ëŠ” 'ë§í¬' íƒìƒ‰
            # ê·¸ëˆ„ë³´ë“œ ëª¨ë°”ì¼ì€ ë³´í†µ <li> ì•ˆì— ì œëª©ê³¼ ë§í¬ê°€ ìˆìŠµë‹ˆë‹¤.
            items = soup.find_all(['li', 'tr', 'a']) 
            
            for item in items:
                # í•´ë‹¹ ì˜ì—­ ì•ˆì— ë§í¬(<a>)ê°€ ìˆëŠ”ì§€ í™•ì¸
                a_tag = item if item.name == 'a' else item.find('a', href=True)
                if not a_tag or 'href' not in a_tag.attrs:
                    continue
                
                href = a_tag['href']
                text = item.get_text(strip=True) # ì•„ì´í…œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ ê²€ì‚¬
                
                # ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ íŒ¨í„´ (wr_id) í™•ì¸
                if 'wr_id=' in href and 'bo_table=' in href:
                    # ë¶ˆí•„ìš”í•œ ë§í¬ ì œì™¸
                    if any(x in href for x in ['write', 'update', 'delete', 'search']):
                        continue
                        
                    full_link = urljoin(url, href)
                    
                    # [í•µì‹¬] í‚¤ì›Œë“œ ë§¤ì¹­ ë¡œì§ (ì œëª© ë˜ëŠ” ì „ì²´ í…ìŠ¤íŠ¸ì— í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸)
                    if not keyword:
                        if full_link not in links: links.append(full_link)
                    elif keyword in text or keyword in full_link:
                        if full_link not in links: links.append(full_link)
            
            # ì¤‘ë³µ ì œê±°
            final_links = list(dict.fromkeys(links))
            st.success(f"ğŸ¯ '{keyword if keyword else 'ì „ì²´'}' í‚¤ì›Œë“œ ê²Œì‹œë¬¼ {len(final_links)}ê°œ ë°œê²¬!")
            return final_links

        except Exception as e:
            st.error(f"âŒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            # ëª¨ë°”ì¼ ê²Œì‹œíŒìš© ë³¸ë¬¸ ì˜ì—­ íƒìƒ‰
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id=\"bo_v_con\") or soup.find(class_=\"view-content\") or soup.body
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-30'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… ì´ˆì •ë°€ ì €ì¸ë§ í¬ë¡¤ëŸ¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
