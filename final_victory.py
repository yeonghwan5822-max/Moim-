import os

target_file = "backend/scripts/crawler.py"

crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin, quote
import urllib3
import re

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, headless=True):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
            'Referer': 'https://m.ebcblue.com/'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        # 1. ì¼ë‹¨ ê²€ìƒ‰ ì¿¼ë¦¬ ì—†ì´ ì „ì²´ ëª©ë¡ì— ë¨¼ì € ì ‘ì†í•©ë‹ˆë‹¤ (ì°¨ë‹¨ ë°©ì§€)
        st.info(f"ğŸŒ ê²Œì‹œíŒ ì ‘ì† ì¤‘: {url}")
        raw_links = self.get_post_links(url, keyword)
        return {'notice': [], 'normal': raw_links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            # SSL ìš°íšŒí•˜ì—¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # [íŠ¹ìˆ˜ ë¡œì§] ëª¨ë“  <a> íƒœê·¸ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë§¤ìš° ë„“ì€ ë²”ìœ„ì˜ íƒìƒ‰ ì‹¤ì‹œ
            all_a = soup.find_all('a', href=True)
            st.write(f"ğŸ” ì „ì²´ íƒìƒ‰ëœ ë§í¬ ìˆ˜: {len(all_a)}ê°œ")

            for a in all_a:
                href = a['href']
                text = a.get_text(strip=True)
                
                # ê²Œì‹œê¸€ ë²ˆí˜¸ê°€ í¬í•¨ëœ ëª¨ë“  ë§í¬ íŒ¨í„´ (wr_id)
                if 'wr_id=' in href:
                    # ê´€ë¦¬ìš© ë§í¬ ì œì™¸
                    if any(x in href for x in ['write', 'update', 'delete', 'search']):
                        continue
                    
                    full_link = urljoin(url, href)
                    
                    # í‚¤ì›Œë“œ 'ì‹¤ìŠµ'ì´ ì œëª©ì´ë‚˜ ë§í¬ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    if not keyword or (keyword in text or keyword in full_link):
                        if full_link not in links:
                            links.append(full_link)
            
            # ë§Œì•½ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ë‹¤ë©´, ê·¸ëˆ„ë³´ë“œ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¥¼ ê°•ì œë¡œ ë¶™ì—¬ì„œ ì¬ì‹œë„
            if not links and keyword:
                st.warning("âš ï¸ ì¼ë°˜ ëª©ë¡ì—ì„œ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                encoded_kw = quote(keyword.encode('utf-8'))
                search_url = f"{url}&sfl=wr_subject||wr_content&stx={encoded_kw}"
                return self.get_post_links(search_url, None) # ì¬ê·€ í˜¸ì¶œ

            st.success(f"ğŸ¯ ìµœì¢… ë°œê²¬ëœ ê²Œì‹œë¬¼: {len(links)}ê°œ")
            return links
        except Exception as e:
            st.error(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content") or soup.body
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True)[:2000] if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-30'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… ì´ˆì •ë°€ ì €ì¸ë§ í¬ë¡¤ëŸ¬ë¡œ êµì²´ ì™„ë£Œ!")
