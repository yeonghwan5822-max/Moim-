import os

# í¬ë¡¤ëŸ¬ íŒŒì¼ ê²½ë¡œ
crawler_path = "backend/scripts/crawler.py"

# ë¡œê·¸ì¸ ê¸°ëŠ¥ + ë©€í‹°í˜¸í¼(ì „ì²´ ìˆœì°°)ê°€ í¬í•¨ëœ ì™„ë²½í•œ í¬ë¡¤ëŸ¬ ì½”ë“œ
crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin
import urllib3
import time

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://m.ebcblue.com/'
        }
        # ë¡œê·¸ì¸ ì£¼ì†Œ
        self.login_url = "https://m.ebcblue.com/bbs/login_check.php"

    # [í•µì‹¬] ë¡œê·¸ì¸ ê¸°ëŠ¥ (ì´ê²Œ ì—†ì–´ì„œ ì—ëŸ¬ê°€ ë‚¬ë˜ ê²ë‹ˆë‹¤!)
    def login(self, user_id, user_pw):
        try:
            login_data = {
                'mb_id': user_id,
                'mb_password': user_pw,
                'url': 'https://m.ebcblue.com/'
            }
            res = self.session.post(self.login_url, data=login_data, headers=self.headers, verify=False)
            
            # ë¡œê·¸ì¸ ì‹¤íŒ¨ ì‹œ ì²´í¬
            if "ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë¦½ë‹ˆë‹¤" in res.text or "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”" in res.text:
                return False
            return True
        except Exception as e:
            print(f"ë¡œê·¸ì¸ ì—ëŸ¬: {e}")
            return False

    def get_categorized_links(self, url, keyword=None):
        # 1. í˜„ì¬ í˜ì´ì§€ ë¨¼ì € ìŠ¤ìº”
        links = self._scan_page(url, keyword)
        if links: 
            return {'notice': [], 'normal': links}
        
        # 2. ì—†ìœ¼ë©´ ì „ì²´ ê²Œì‹œíŒ ìˆœì°° (ë¡œê·¸ì¸ ìƒíƒœ ìœ ì§€)
        st.info("í˜„ì¬ í˜ì´ì§€ì— ê¸€ì´ ì—†ì–´, ì „ì²´ ê²Œì‹œíŒì„ ìˆœì°°í•©ë‹ˆë‹¤...")
        boards = self._find_boards(url)
        
        all_links = []
        progress = st.progress(0)
        status_text = st.empty()
        
        for i, board in enumerate(boards):
            if not board: continue
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            board_name = board.split('bo_table=')[-1]
            status_text.text(f"ğŸƒ ì´ë™ ì¤‘: {board_name} ê²Œì‹œíŒ...")
            progress.progress((i + 1) / len(boards))
            
            # ë‹¬ë ¥ ë“±ì€ ê±´ë„ˆë›°ê¸°
            if 'calendar' in board: continue
            
            # ê° ê²Œì‹œíŒ ìŠ¤ìº”
            found = self._scan_page(board, keyword, silent=True)
            if found:
                all_links.extend(found)
        
        status_text.empty()
        progress.empty()
        
        return {'notice': [], 'normal': list(set(all_links))}

    def _find_boards(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            boards = []
            for a in soup.find_all('a', href=True):
                if 'board.php' in a['href'] and 'bo_table=' in a['href']:
                    boards.append(urljoin(url, a['href']))
            return list(set(boards))
        except: return []

    def _scan_page(self, url, keyword, silent=False):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            
            for a in soup.find_all('a', href=True):
                href = a['href']
                # ë¡œê·¸ì¸ ìƒíƒœì—ì„œëŠ” wr_id ë§í¬ê°€ ë³´ì…ë‹ˆë‹¤!
                if 'wr_id=' in href and 'bo_table=' in href:
                    if any(x in href for x in ['write', 'update', 'delete', 'search']): continue
                    
                    full_link = urljoin(url, href)
                    text = a.get_text(strip=True)
                    
                    if keyword:
                        if keyword in text or keyword in full_link:
                            links.append(full_link)
                    else:
                        links.append(full_link)
            return list(set(links))
        except: return []

    def get_post_content(self, url):
        return {'title': '', 'content': ''}
"""

with open(crawler_path, "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… í¬ë¡¤ëŸ¬ì— ë¡œê·¸ì¸ ê¸°ëŠ¥ì„ ì„±ê³µì ìœ¼ë¡œ ì´ì‹í–ˆìŠµë‹ˆë‹¤!")import os

# í¬ë¡¤ëŸ¬ íŒŒì¼ ê²½ë¡œ
crawler_path = "backend/scripts/crawler.py"

# ë¡œê·¸ì¸ ê¸°ëŠ¥ + ë©€í‹°í˜¸í¼(ì „ì²´ ìˆœì°°)ê°€ í¬í•¨ëœ ì™„ë²½í•œ í¬ë¡¤ëŸ¬ ì½”ë“œ
crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin
import urllib3
import time

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://m.ebcblue.com/'
        }
        # ë¡œê·¸ì¸ ì£¼ì†Œ
        self.login_url = "https://m.ebcblue.com/bbs/login_check.php"

    # [í•µì‹¬] ë¡œê·¸ì¸ ê¸°ëŠ¥ (ì´ê²Œ ì—†ì–´ì„œ ì—ëŸ¬ê°€ ë‚¬ë˜ ê²ë‹ˆë‹¤!)
    def login(self, user_id, user_pw):
        try:
            login_data = {
                'mb_id': user_id,
                'mb_password': user_pw,
                'url': 'https://m.ebcblue.com/'
            }
            res = self.session.post(self.login_url, data=login_data, headers=self.headers, verify=False)
            
            # ë¡œê·¸ì¸ ì‹¤íŒ¨ ì‹œ ì²´í¬
            if "ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë¦½ë‹ˆë‹¤" in res.text or "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”" in res.text:
                return False
            return True
        except Exception as e:
            print(f"ë¡œê·¸ì¸ ì—ëŸ¬: {e}")
            return False

    def get_categorized_links(self, url, keyword=None):
        # 1. í˜„ì¬ í˜ì´ì§€ ë¨¼ì € ìŠ¤ìº”
        links = self._scan_page(url, keyword)
        if links: 
            return {'notice': [], 'normal': links}
        
        # 2. ì—†ìœ¼ë©´ ì „ì²´ ê²Œì‹œíŒ ìˆœì°° (ë¡œê·¸ì¸ ìƒíƒœ ìœ ì§€)
        st.info("í˜„ì¬ í˜ì´ì§€ì— ê¸€ì´ ì—†ì–´, ì „ì²´ ê²Œì‹œíŒì„ ìˆœì°°í•©ë‹ˆë‹¤...")
        boards = self._find_boards(url)
        
        all_links = []
        progress = st.progress(0)
        status_text = st.empty()
        
        for i, board in enumerate(boards):
            if not board: continue
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            board_name = board.split('bo_table=')[-1]
            status_text.text(f"ğŸƒ ì´ë™ ì¤‘: {board_name} ê²Œì‹œíŒ...")
            progress.progress((i + 1) / len(boards))
            
            # ë‹¬ë ¥ ë“±ì€ ê±´ë„ˆë›°ê¸°
            if 'calendar' in board: continue
            
            # ê° ê²Œì‹œíŒ ìŠ¤ìº”
            found = self._scan_page(board, keyword, silent=True)
            if found:
                all_links.extend(found)
        
        status_text.empty()
        progress.empty()
        
        return {'notice': [], 'normal': list(set(all_links))}

    def _find_boards(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            boards = []
            for a in soup.find_all('a', href=True):
                if 'board.php' in a['href'] and 'bo_table=' in a['href']:
                    boards.append(urljoin(url, a['href']))
            return list(set(boards))
        except: return []

    def _scan_page(self, url, keyword, silent=False):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            
            for a in soup.find_all('a', href=True):
                href = a['href']
                # ë¡œê·¸ì¸ ìƒíƒœì—ì„œëŠ” wr_id ë§í¬ê°€ ë³´ì…ë‹ˆë‹¤!
                if 'wr_id=' in href and 'bo_table=' in href:
                    if any(x in href for x in ['write', 'update', 'delete', 'search']): continue
                    
                    full_link = urljoin(url, href)
                    text = a.get_text(strip=True)
                    
                    if keyword:
                        if keyword in text or keyword in full_link:
                            links.append(full_link)
                    else:
                        links.append(full_link)
            return list(set(links))
        except: return []

    def get_post_content(self, url):
        return {'title': '', 'content': ''}
"""

with open(crawler_path, "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… í¬ë¡¤ëŸ¬ì— ë¡œê·¸ì¸ ê¸°ëŠ¥ì„ ì„±ê³µì ìœ¼ë¡œ ì´ì‹í–ˆìŠµë‹ˆë‹¤!")
"""

---

### ğŸš€ 2ë‹¨ê³„: ì‹¤í–‰ ë° ë°°í¬ (í„°ë¯¸ë„)

1.  **íŒ¨ì¹˜ ì‹¤í–‰:**
    ```bash
    python3 complete_repair.py
    ```
2.  **ê¹ƒí—ˆë¸Œ ë°°í¬:**
    ```bash
    git add .
    git commit -m "Fix: í¬ë¡¤ëŸ¬ì— ëˆ„ë½ëœ ë¡œê·¸ì¸(login) í•¨ìˆ˜ ì¶”ê°€"
    git push
    ```

---

### ğŸ 3ë‹¨ê³„: ìµœì¢… í…ŒìŠ¤íŠ¸

1.  **Streamlit Cloud**ì—ì„œ **[Reboot app]**ì„ í´ë¦­í•©ë‹ˆë‹¤.
2.  ì•±ì´ ì¼œì§€ë©´ **ë¡œê·¸ì¸ ì •ë³´ë¥¼ ì…ë ¥**í•˜ê³  ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.

**[ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤]**
* ì•„ê¹Œ ë–´ë˜ `AttributeError`ê°€ ì‚¬ë¼ì§€ê³ ,
* **"ğŸ”‘ ë¡œê·¸ì¸ ì‹œë„ ì¤‘..."**ì´ë¼ëŠ” ë©”ì‹œì§€ê°€ ëœ¬ ë’¤,
* **"âœ… ë¡œê·¸ì¸ ì„±ê³µ!"**ê³¼ í•¨ê»˜ ê²Œì‹œê¸€ ëª©ë¡ì´ ë‚˜íƒ€ë‚  ê²ƒì…ë‹ˆë‹¤.

ì´ì œ ì •ë§ ì½”ì•ì…ë‹ˆë‹¤! ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ë¡œê·¸ì¸ í•¨ìˆ˜ê°€ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤. ê²°ê³¼ ì•Œë ¤ì£¼ì„¸ìš”!

