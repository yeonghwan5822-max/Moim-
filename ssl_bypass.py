import os

target_file = "backend/scripts/crawler.py"

crawler_code = """import requests
from bs4 import BeautifulSoup
import time
import streamlit as st
from urllib.parse import urljoin, quote
import urllib3

# [í•µì‹¬] SSL ê²½ê³  ë©”ì‹œì§€ ë„ê¸°
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, headless=True):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        # ê²€ìƒ‰ URL ìƒì„± ë¡œì§
        final_url = url
        if keyword:
            encoded_kw = quote(keyword.encode('utf-8'))
            sep = "&" if "?" in url else "?"
            final_url = f"{url}{sep}sfl=wr_subject||wr_content&stx={encoded_kw}"
        
        st.write(f"ğŸŒ **ì ‘ì† ì‹œë„ (SSL ìš°íšŒ):** {final_url}")
        raw_links = self.get_post_links(final_url, keyword)
        return {'notice': [], 'normal': raw_links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            # [í•µì‹¬] verify=False ë¥¼ ì¶”ê°€í•˜ì—¬ SSL ì¸ì¦ì„œ ê²€ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.
            res = self.session.get(url, headers=self.headers, timeout=15, verify=False)
            res.raise_for_status()
            
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ë” ì •ë°€í•œ ë§í¬ ì¶”ì¶œ (ê·¸ëˆ„ë³´ë“œ ëª¨ë°”ì¼ ê²Œì‹œíŒìš©)
            all_a = soup.find_all('a', href=True)
            for a in all_a:
                href = a['href']
                text = a.get_text(strip=True)
                
                # wr_idê°€ í¬í•¨ëœ ì‹¤ì œ ê²Œì‹œê¸€ ë§í¬ë§Œ í•„í„°ë§
                if 'wr_id=' in href:
                    if any(x in href for x in ['write', 'update', 'delete']): continue
                    
                    full_link = urljoin(url, href)
                    if full_link not in links:
                        # í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´ ì œëª©/ë§í¬ ëŒ€ì¡°
                        if not keyword or (keyword in text or keyword in full_link):
                            links.append(full_link)
            
            st.success(f"ğŸ¯ **'{keyword if keyword else 'ì „ì²´'}'** ê²€ìƒ‰ ê²°ê³¼: {len(links)}ê°œ ë°œê²¬")
            return links
        except Exception as e:
            st.error(f"âŒ ì ‘ì† ì‹¤íŒ¨: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find('h1') or soup.find('h2') or soup.find('title')
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content")
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True)[:1500] if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-30'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… SSL ìš°íšŒ íŒ¨ì¹˜ ì™„ë£Œ!")

