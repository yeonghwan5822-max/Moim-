import os

# íŒŒì¼ ê²½ë¡œ: ë©”ì¸ ì•± íŒŒì¼ì— ëª¨ë“  ê¸°ëŠ¥ì„ ëª°ì•„ë„£ìŠµë‹ˆë‹¤.
target_file = "backend/streamlit_app.py"

fusion_code = """import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import urllib3

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# 1. ë‚´ì¥ í¬ë¡¤ëŸ¬ (EbcCrawler) ì •ì˜
# ==========================================
class EbcCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://m.ebcblue.com/'
        }
        self.login_url = "https://m.ebcblue.com/bbs/login_check.php"

    def login(self, user_id, user_pw):
        try:
            data = {'mb_id': user_id, 'mb_password': user_pw, 'url': 'https://m.ebcblue.com/'}
            res = self.session.post(self.login_url, data=data, headers=self.headers, verify=False)
            if "ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë¦½ë‹ˆë‹¤" in res.text or "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”" in res.text:
                return False
            return True
        except: return False

    def get_links(self, url, keyword=None):
        # 1. í˜„ì¬ í˜ì´ì§€ ìŠ¤ìº”
        links = self._scan(url, keyword)
        if links: return links

        # 2. ì—†ìœ¼ë©´ ì „ì²´ ê²Œì‹œíŒ ìë™ ìˆœì°°
        st.info("í˜„ì¬ í˜ì´ì§€ì— ê¸€ì´ ì—†ì–´, ì „ì²´ ê²Œì‹œíŒì„ ìˆœì°°í•©ë‹ˆë‹¤...")
        boards = self._find_boards(url)
        all_links = []
        
        status = st.empty()
        prog = st.progress(0)
        
        for i, board in enumerate(boards):
            if 'calendar' in board: continue
            
            board_name = board.split('bo_table=')[-1]
            status.text(f"ğŸƒ {board_name} ê²Œì‹œíŒ í™•ì¸ ì¤‘...")
            prog.progress((i+1)/len(boards))
            
            found = self._scan(board, keyword)
            if found: all_links.extend(found)
            
        status.empty()
        prog.empty()
        return list(set(all_links))

    def _find_boards(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            boards = []
            for a in soup.find_all('a', href=True):
                if 'board.php' in a['href'] and 'bo_table=' in a['href']:
                    boards.append(urljoin(url, a['href']))
            return list(set(boards))
        except: return []

    def _scan(self, url, keyword):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            for a in soup.find_all('a', href=True):
                href = a['href']
                if 'wr_id=' in href and 'bo_table=' in href:
                    if any(x in href for x in ['write', 'update', 'delete', 'search']): continue
                    full_link = urljoin(url, href)
                    text = a.get_text(strip=True)
                    if keyword:
                        if keyword in text or keyword in full_link: links.append(full_link)
                    else:
                        links.append(full_link)
            return links
        except: return []

# ==========================================
# 2. í™”ë©´ (UI) êµ¬ì„±
# ==========================================
st.set_page_config(page_title="MOIM ë²ˆì—­ê¸° (Final)", layout="wide")
st.title("ğŸ” MOIM ë²ˆì—­ê¸° : íšŒì› ì „ìš© ëª¨ë“œ")

with st.sidebar:
    st.header("ë¡œê·¸ì¸")
    user_id = st.text_input("ì•„ì´ë””", key="uid")
    user_pw = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="upw")

url = st.text_input("íƒ€ê²Ÿ URL", "http://m.ebcblue.com/")
keyword = st.text_input("í‚¤ì›Œë“œ (ì„ íƒ)")

if st.button("ğŸš€ ë¡œê·¸ì¸í•˜ê³  ê²Œì‹œë¬¼ ì°¾ê¸°"):
    if not user_id or not user_pw:
        st.error("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¡œê·¸ì¸ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”.")
    else:
        crawler = EbcCrawler() # ìœ„ì—ì„œ ì •ì˜í•œ í´ë˜ìŠ¤ ë°”ë¡œ ì‚¬ìš©
        with st.spinner("ë¡œê·¸ì¸ ì¤‘..."):
            if crawler.login(user_id, user_pw):
                st.success("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
                
                results = crawler.get_links(url, keyword)
                if results:
                    st.success(f"ğŸ‰ {len(results)}ê°œì˜ ê²Œì‹œë¬¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                    for link in results:
                        st.write(f"- {link}")
                else:
                    st.warning("ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨. ì•„ì´ë””/ë¹„ë²ˆì„ í™•ì¸í•˜ì„¸ìš”.")
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(fusion_code)

print("âœ… [í†µí•© ì™„ë£Œ] ì´ì œ íŒŒì¼ ì¶©ëŒë¡œ ì¸í•œ ì—ëŸ¬ëŠ” ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
