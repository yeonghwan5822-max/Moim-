import os

target_file = "backend/scripts/crawler.py"

crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin, quote, urlparse, parse_qs
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://m.ebcblue.com/'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        st.info(f"ğŸ“¡ ë¶„ì„ ì‹œì‘: {url}")
        
        # 1ì°¨ ì‹œë„: í˜„ì¬ í˜ì´ì§€ì—ì„œ ë°”ë¡œ ì°¾ê¸°
        links = self.get_post_links(url, keyword, depth=0)
        
        # 2ì°¨ ì‹œë„: ì—†ìœ¼ë©´ ê²Œì‹œíŒì„ ì°¾ì•„ì„œ ë“¤ì–´ê°€ê¸° (ììœ¨ì£¼í–‰)
        if not links:
            st.warning("âš ï¸ í˜„ì¬ í˜ì´ì§€ì— ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤. í•˜ìœ„ ê²Œì‹œíŒì„ íƒìƒ‰í•©ë‹ˆë‹¤...")
            board_links = self._find_board_links(url)
            
            if board_links:
                target_board = board_links[0] # ì²« ë²ˆì§¸ ê²Œì‹œíŒ ì„ íƒ
                st.success(f"ğŸš€ [ììœ¨ì£¼í–‰] ë°œê²¬ëœ ê²Œì‹œíŒìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤: {target_board}")
                links = self.get_post_links(target_board, keyword, depth=1)
            else:
                st.error("âŒ ì´ë™í•  ìˆ˜ ìˆëŠ” ê²Œì‹œíŒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        return {'notice': [], 'normal': links}

    def _find_board_links(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            boards = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                if 'board.php' in href and 'bo_table=' in href:
                    full_link = urljoin(url, href)
                    if full_link not in boards:
                        boards.append(full_link)
            return boards
        except: return []

    def get_post_links(self, url, keyword=None, depth=0):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ì§„ë‹¨ ë¡œê·¸ (ë©”ì¸ í˜ì´ì§€ì¼ ë•Œë§Œ ì¶œë ¥)
            if depth == 0:
                all_a = soup.find_all('a', href=True)
                with st.expander(f"ğŸ•µï¸â€â™‚ï¸ í˜ì´ì§€ ì§„ë‹¨ (ë§í¬ {len(all_a)}ê°œ)", expanded=True):
                    for a in all_a[:3]:
                        st.text(f"[{a.get_text(strip=True)}] -> {a['href']}")

            for a in soup.find_all('a', href=True):
                href = a['href']
                text = a.get_text(strip=True)
                full_link = urljoin(url, href)
                
                # wr_id (ê²Œì‹œê¸€) ì°¾ê¸°
                if 'wr_id=' in href:
                    if any(x in href for x in ['write', 'update', 'delete', 'search', 'login']): continue
                    
                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if keyword:
                        if keyword.lower() in text.lower() or keyword.lower() in full_link.lower():
                            if full_link not in links: links.append(full_link)
                    else:
                        if full_link not in links: links.append(full_link)
            
            if links:
                st.success(f"ğŸ¯ {len(links)}ê°œì˜ ê²Œì‹œê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤! (ì¶œì²˜: {url})")
            
            return links
        except Exception as e:
            if depth == 0: st.error(f"âŒ ì˜¤ë¥˜: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content") or soup.body
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-31'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… ììœ¨ì£¼í–‰(Auto-Pilot) ì‹œìŠ¤í…œ íƒ‘ì¬ ì™„ë£Œ!")
