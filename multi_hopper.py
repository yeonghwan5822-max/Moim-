import os

target_file = "backend/scripts/crawler.py"

crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin
import urllib3
import time

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
            'Referer': 'https://m.ebcblue.com/'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        st.info(f"ğŸ“¡ íƒìƒ‰ ì‹œì‘: {url}")
        
        # 1. ì…ë ¥ëœ URLì—ì„œ ë°”ë¡œ ì‹œë„
        links = self.get_post_links(url, keyword, silent=True)
        if links:
            return {'notice': [], 'normal': links}
            
        # 2. ì‹¤íŒ¨ ì‹œ, ë©”ì¸ í˜ì´ì§€ì—ì„œ ë‹¤ë¥¸ ê²Œì‹œíŒ ëª©ë¡ ìˆ˜ì§‘ (ë©€í‹° í˜¸í¼ ê°€ë™)
        st.warning("âš ï¸ í˜„ì¬ í˜ì´ì§€ì—ì„œ ì†Œë“ì´ ì—†ì–´, ë‹¤ë¥¸ ê²Œì‹œíŒë“¤ì„ ìˆœì°°í•©ë‹ˆë‹¤...")
        board_list = self._find_all_boards(url)
        
        if not board_list:
            st.error("âŒ ì´ë™í•  ìˆ˜ ìˆëŠ” ê²Œì‹œíŒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {'notice': [], 'normal': []}

        # 3. ë°œê²¬ëœ ê²Œì‹œíŒë“¤ì„ í•˜ë‚˜ì”© ìˆœíšŒ
        success_links = []
        progress_bar = st.progress(0)
        
        for i, board_url in enumerate(board_list):
            # 'calendar'(ë‹¬ë ¥) ê°™ì€ íŠ¹ìˆ˜ ê²Œì‹œíŒì€ ê±´ë„ˆë›°ê¸° (íš¨ìœ¨ì„±)
            if 'calendar' in board_url: continue
            
            # ì§„í–‰ë¥  í‘œì‹œ
            progress_bar.progress((i + 1) / len(board_list))
            st.write(f"ğŸƒ ì´ë™ ì¤‘: ...{board_url[-20:]}")
            
            # ì ‘ì† ì‹œë„
            found_links = self.get_post_links(board_url, keyword, silent=True)
            
            if found_links:
                st.success(f"ğŸ‰ ì°¾ì•˜ë‹¤! [ê²Œì‹œíŒ: {board_url}]ì—ì„œ {len(found_links)}ê°œ ë°œê²¬!")
                success_links = found_links
                break # í•˜ë‚˜ë¼ë„ ì°¾ìœ¼ë©´ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ê³  ê²°ê³¼ ë°˜í™˜
            
            time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€

        progress_bar.empty()
        
        if not success_links:
            st.error("ğŸ˜­ ëª¨ë“  ë°©ì„ ë‹¤ ë’¤ì¡ŒëŠ”ë° ê²Œì‹œê¸€ì„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
        return {'notice': [], 'normal': success_links}

    def _find_all_boards(self, url):
        # ë©”ì¸ í˜ì´ì§€ì—ì„œ 'board.php'ê°€ ë“¤ì–´ê°„ ëª¨ë“  ë§í¬ ì¶”ì¶œ
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            boards = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                if 'board.php' in href and 'bo_table=' in href:
                    full_link = urljoin(url, href)
                    if full_link not in boards:
                        boards.append(full_link)
            
            st.info(f"ğŸ” ì´ {len(boards)}ê°œì˜ ê²Œì‹œíŒ ì…êµ¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            return boards
        except: return []

    def get_post_links(self, url, keyword=None, silent=False):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            
            for a in soup.find_all('a', href=True):
                href = a['href']
                # wr_id(ê²Œì‹œê¸€) íŒ¨í„´ í™•ì¸
                if 'wr_id=' in href:
                    if any(x in href for x in ['write', 'update', 'delete', 'search', 'login']): continue
                    
                    full_link = urljoin(url, href)
                    text = a.get_text(strip=True)
                    
                    if keyword:
                        if keyword.lower() in text.lower() or keyword.lower() in full_link.lower():
                            if full_link not in links: links.append(full_link)
                    else:
                        if full_link not in links: links.append(full_link)
            
            if not silent and links:
                st.success(f"ğŸ¯ {len(links)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                
            return links
        except: return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content") or soup.body
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-31'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… ë‹¤ì¤‘ ê²Œì‹œíŒ ìˆœì°° ì‹œìŠ¤í…œ(Multi-Hopper) ì¥ì°© ì™„ë£Œ!")

