import os

# 1. ì„¤ì¹˜ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•œ ê¹¨ë—í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡ (requirements.txt)
# ì„¤ì¹˜ ì‹¤íŒ¨ í™•ë¥ ì´ ë†’ì€ ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì¼ë‹¨ ì œì™¸í•˜ê³  í•„ìˆ˜ í•­ëª©ë§Œ ë„£ì—ˆìŠµë‹ˆë‹¤.
requirements = """streamlit
requests
beautifulsoup4
python-dotenv
urllib3
"""

# 2. í¬ë¡¤ëŸ¬ ì½”ë“œ (crawler.py) - SSL ìš°íšŒ ë° ë²”ìš© ê²€ìƒ‰ ê¸°ëŠ¥ í¬í•¨
crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        self.headers = {'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1'}

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        st.info(f"ğŸŒ íƒìƒ‰ ì¤‘: {url}")
        return {'notice': [], 'normal': self.get_post_links(url, keyword)}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            soup = BeautifulSoup(res.content, 'html.parser')
            for a in soup.find_all('a', href=True):
                href = a['href']
                if 'wr_id=' in href and 'bo_table=' in href:
                    if any(x in href for x in ['write', 'update', 'delete']): continue
                    full_link = urljoin(url, href)
                    if not keyword or (keyword.lower() in a.get_text().lower() or keyword.lower() in full_link.lower()):
                        if full_link not in links: links.append(full_link)
            st.success(f"ğŸ¯ {len(links)}ê°œì˜ ê²Œì‹œë¬¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return links
        except Exception as e:
            st.error(f"âŒ ì ‘ì† ì—ëŸ¬: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            soup = BeautifulSoup(res.content, 'html.parser')
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content")
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-31'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

# íŒŒì¼ ì“°ê¸°
os.makedirs("backend/scripts", exist_ok=True)
with open("backend/requirements.txt", "w", encoding="utf-8") as f:
    f.write(requirements)
with open("backend/scripts/crawler.py", "w", encoding="utf-8") as f:
    f.write(crawler_code)
# ì„¤ì¹˜ ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” packages.txtëŠ” ë¹„ì›ë‹ˆë‹¤.
with open("backend/packages.txt", "w") as f:
    f.write("")

print("âœ… ë³µêµ¬ ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ë°°í¬ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
