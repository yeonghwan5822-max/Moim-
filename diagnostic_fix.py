import os

# í¬ë¡¤ëŸ¬ ì½”ë“œ (crawler.py) - HTML êµ¬ì¡° ì§„ë‹¨ ë° ê°•ì œ ì¶”ì¶œ ëª¨ë“œ
crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        st.info(f"ğŸ“¡ ë¶„ì„ ì‹œì‘: {url}")
        links = self.get_post_links(url, keyword)
        return {'notice': [], 'normal': links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # [ì§„ë‹¨ ë¦¬í¬íŠ¸] ì‚¬ì´íŠ¸ê°€ ë¡œê·¸ì¸ í˜ì´ì§€ì¸ì§€, ë¹ˆ í˜ì´ì§€ì¸ì§€ í™•ì¸
            with st.expander("ğŸ” ì‚¬ì´íŠ¸ ë‚´ë¶€ íˆ¬ì‹œê²½ (ì—¬ê¸°ë¥¼ í´ë¦­í•˜ì„¸ìš”!)", expanded=True):
                st.write(f"**ì‘ë‹µ ì½”ë“œ:** {res.status_code}")
                st.write(f"**í˜ì´ì§€ ì œëª©:** {soup.title.string if soup.title else 'ì œëª© ì—†ìŒ'}")
                
                # í˜ì´ì§€ì— ìˆëŠ” ëª¨ë“  ë§í¬ë¥¼ ì‹¹ ë‹¤ ê¸ì–´ë´…ë‹ˆë‹¤.
                all_tags = soup.find_all('a', href=True)
                st.write(f"**ë°œê²¬ëœ ì´ ë§í¬ ìˆ˜:** {len(all_tags)}ê°œ")
                
                # ë§í¬ ìƒ˜í”Œ 5ê°œ ì¶œë ¥
                if all_tags:
                    st.write("ğŸ”— **ë§í¬ ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):**")
                    for i, tag in enumerate(all_tags[:5]):
                        st.code(f"{tag.get_text(strip=True)} -> {tag['href']}")

            # [ê°•ì œ ì¶”ì¶œ] wr_id ì¡°ê±´ì´ ì•ˆ ë§ìœ¼ë©´ 'board.php'ê°€ ë“¤ì–´ê°„ ëª¨ë“  ë§í¬ë¥¼ ìˆ˜ì§‘
            for a in soup.find_all('a', href=True):
                href = a['href']
                text = a.get_text(strip=True)
                full_link = urljoin(url, href)

                # ì¡°ê±´ 1: wr_idê°€ ìˆëŠ” ì •ì„ ê²Œì‹œê¸€
                if 'wr_id=' in href:
                     if not any(x in href for x in ['write', 'update', 'delete', 'search']):
                        if self._check_keyword(keyword, text, full_link):
                            if full_link not in links: links.append(full_link)
                
                # ì¡°ê±´ 2 (ë¹„ìƒìš©): wr_idëŠ” ì—†ì§€ë§Œ ê²Œì‹œíŒ ë§í¬ì²˜ëŸ¼ ìƒê¸´ ê²ƒ
                elif 'board.php' in href and 'bo_table' in href:
                     if self._check_keyword(keyword, text, full_link):
                        if full_link not in links: links.append(full_link)

            if not links:
                st.error("âš ï¸ ê²Œì‹œê¸€ë¡œ ì¶”ì •ë˜ëŠ” ë§í¬ë¥¼ í•˜ë‚˜ë„ ëª» ê±´ì¡ŒìŠµë‹ˆë‹¤. ìœ„ 'íˆ¬ì‹œê²½' ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                st.success(f"ğŸ¯ {len(links)}ê°œì˜ ê²Œì‹œê¸€ í™•ë³´ ì„±ê³µ!")
                
            return links

        except Exception as e:
            st.error(f"âŒ ì ‘ì† ì—ëŸ¬: {e}")
            return []

    def _check_keyword(self, keyword, text, link):
        if not keyword: return True
        return (keyword.lower() in text.lower() or keyword.lower() in link.lower())

    def get_post_content(self, url):
        # ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§ì€ ë™ì¼
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content") or soup.body
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-31'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open("backend/scripts/crawler.py", "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… ì§„ë‹¨ìš© íˆ¬ì‹œê²½ í¬ë¡¤ëŸ¬ ì„¤ì¹˜ ì™„ë£Œ!")
