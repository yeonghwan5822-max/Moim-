import os

target_file = "backend/streamlit_app.py"

fusion_code = """import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import urllib3

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://m.ebcblue.com/'
        }
        self.login_url = "https://m.ebcblue.com/bbs/login_check.php"

    def login(self, user_id, user_pw):
        try:
            data = {'mb_id': user_id, 'mb_password': user_pw, 'url': 'https://m.ebcblue.com/'}
            res = self.session.post(self.login_url, data=data, headers=self.headers, verify=False)
            if "ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë¦½ë‹ˆë‹¤" in res.text or "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”" in res.text:
                return False
            return True
        except: return False

    def get_links(self, url, keyword=None, fetch_all_home=True):
        all_links = []
        
        # 1. [í•µì‹¬] í™ˆ í™”ë©´ ì‹¹ì“¸ì´ (í‚¤ì›Œë“œ ë¬´ì‹œ ì˜µì…˜)
        st.info("ğŸ  í™ˆ í™”ë©´ì— ë³´ì´ëŠ” ê²Œì‹œê¸€ì„ ìš°ì„  ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        home_links = self._scan(url, keyword if not fetch_all_home else None)
        
        if home_links:
            st.success(f"âœ… í™ˆ í™”ë©´ì—ì„œ {len(home_links)}ê°œì˜ ìµœì‹  ê¸€ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤!")
            all_links.extend(home_links)
        else:
            st.warning("í™ˆ í™”ë©´ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²Œì‹œíŒ ìˆœì°°ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")

        # 2. ì „ì²´ ê²Œì‹œíŒ ìë™ ìˆœì°°
        st.divider()
        st.info("ğŸ” ë‚˜ë¨¸ì§€ ê²Œì‹œíŒë“¤ì„ ìˆœì°°í•©ë‹ˆë‹¤...")
        boards = self._find_boards(url)
        
        status = st.empty()
        prog = st.progress(0)
        
        for i, board in enumerate(boards):
            if 'calendar' in board: continue
            
            board_name = board.split('bo_table=')[-1]
            status.text(f"ğŸƒ {board_name} ê²Œì‹œíŒ í™•ì¸ ì¤‘...")
            prog.progress((i+1)/len(boards))
            
            # ê²Œì‹œíŒ ë‚´ë¶€ ê¸€ì€ í‚¤ì›Œë“œ ì ìš©
            found = self._scan(board, keyword)
            if found: all_links.extend(found)
            
        status.empty()
        prog.empty()
        
        # ì¤‘ë³µ ì œê±° í›„ ë°˜í™˜
        return list(set(all_links))

    def _find_boards(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            boards = []
            for a in soup.find_all('a', href=True):
                if 'board.php' in a['href'] and 'bo_table=' in a['href']:
                    boards.append(urljoin(url, a['href']))
            return list(set(boards))
        except: return []

    def _scan(self, url, keyword):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            
            for a in soup.find_all('a', href=True):
                href = a['href']
                # wr_idê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ê²Œì‹œê¸€ë¡œ ê°„ì£¼
                if 'wr_id=' in href:
                    # ê¸€ì“°ê¸°, ìˆ˜ì •, ì‚­ì œ ë“± ë¶ˆí•„ìš”í•œ ë§í¬ ì œì™¸
                    if any(x in href for x in ['write', 'update', 'delete', 'search']): continue
                    
                    full_link = urljoin(url, href)
                    text = a.get_text(strip=True)
                    
                    if keyword:
                        # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì œëª©ì´ë‚˜ ë§í¬ì— í¬í•¨ë˜ì–´ì•¼ í•¨
                        if keyword in text or keyword in full_link: links.append(full_link)
                    else:
                        # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ ìˆ˜ì§‘
                        links.append(full_link)
            return links
        except: return []

# ==========================================
# UI êµ¬ì„±
# ==========================================
st.set_page_config(page_title="MOIM ë²ˆì—­ê¸° (Final)", layout="wide")
st.title("ğŸ” MOIM ë²ˆì—­ê¸° : ì‹¹ì“¸ì´ ëª¨ë“œ")

with st.sidebar:
    st.header("ë¡œê·¸ì¸")
    user_id = st.text_input("ì•„ì´ë””", key="uid")
    user_pw = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="upw")

st.write("### 1. í¬ë¡¤ë§ ì„¤ì •")
col1, col2 = st.columns([3, 1])
with col1:
    url = st.text_input("íƒ€ê²Ÿ URL", "http://m.ebcblue.com/")
with col2:
    keyword = st.text_input("í‚¤ì›Œë“œ (ì„ íƒì‚¬í•­)", placeholder="ì˜ˆ: ê³µì§€")

# [ìƒˆ ê¸°ëŠ¥] ì²´í¬ë°•ìŠ¤ ì¶”ê°€
fetch_all_home = st.checkbox("âœ… í™ˆ í™”ë©´ì— ë³´ì´ëŠ” ê¸€ì€ í‚¤ì›Œë“œ ìƒê´€ì—†ì´ ë‹¤ ê°€ì ¸ì˜¤ê¸°", value=True)

if st.button("ğŸš€ ë¡œê·¸ì¸í•˜ê³  ê²Œì‹œë¬¼ ì°¾ê¸°"):
    if not user_id or not user_pw:
        st.error("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¡œê·¸ì¸ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”.")
    else:
        crawler = EbcCrawler()
        with st.spinner("ë¡œê·¸ì¸ ì¤‘..."):
            if crawler.login(user_id, user_pw):
                st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
                
                # ì²´í¬ë°•ìŠ¤ ì˜µì…˜(fetch_all_home) ì „ë‹¬
                results = crawler.get_links(url, keyword, fetch_all_home)
                
                if results:
                    st.success(f"ğŸ‰ ì´ {len(results)}ê°œì˜ ê²Œì‹œë¬¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                    st.write("---")
                    for link in results:
                        st.write(f"- {link}")
                else:
                    st.error("ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ ì§€ìš°ê³  ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
            else:
                st.error("ë¡œê·¸ì¸ ì‹¤íŒ¨. ì•„ì´ë””/ë¹„ë²ˆì„ í™•ì¸í•˜ì„¸ìš”.")
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(fusion_code)

print("âœ… [í™ˆ í™”ë©´ ì‹¹ì“¸ì´] ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
