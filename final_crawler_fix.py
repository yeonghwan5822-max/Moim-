import os

# í¬ë¡¤ëŸ¬ ì½”ë“œ (crawler.py) - ì €ì¸ë§ì‹ ì¶”ì¶œ + í•œê¸€ ê¹¨ì§ ë°©ì§€
crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        # í™”ë©´ì— ì§„í–‰ ìƒí™© í‘œì‹œ
        status = st.empty()
        status.info(f"ğŸ“¡ ì‚¬ì´íŠ¸ ì ‘ì† ì¤‘: {url}")
        
        links = self.get_post_links(url, keyword)
        
        status.empty() # ì™„ë£Œë˜ë©´ ìƒíƒœì°½ ì§€ì›€
        return {'notice': [], 'normal': links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            # [í•µì‹¬] í•œê¸€ ê¹¨ì§ ë°©ì§€ (GnuBoard íŠ¹ì„±)
            res.encoding = res.apparent_encoding 
            
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ì§„ë‹¨ìš© ë¡œê·¸ (í™•ì¥ ë©”ë‰´ë¡œ ìˆ¨ê¹€)
            with st.expander("ğŸ•µï¸â€â™‚ï¸ í¬ë¡¤ë§ ìƒì„¸ ì§„ë‹¨ ë¡œê·¸ (í´ë¦­í•˜ì—¬ ì—´ê¸°)"):
                st.write(f"ì ‘ì† ìƒíƒœ ì½”ë“œ: {res.status_code}")
                all_a = soup.find_all('a', href=True)
                st.write(f"í˜ì´ì§€ ë‚´ ì´ ë§í¬ ìˆ˜: {len(all_a)}ê°œ")
                
                match_count = 0
                for a in all_a:
                    href = a['href']
                    text = a.get_text(strip=True)
                    
                    # [ì¡°ê±´ ì™„í™”] bo_table í™•ì¸ ì œê±°, wr_idë§Œ ìˆìœ¼ë©´ ê²Œì‹œê¸€ë¡œ ê°„ì£¼
                    if 'wr_id=' in href:
                        # ê´€ë¦¬ì/ì‹œìŠ¤í…œ ë§í¬ ì œì™¸
                        if any(x in href for x in ['write', 'update', 'delete', 'search', 'login']):
                            continue
                            
                        full_link = urljoin(url, href)
                        
                        # í‚¤ì›Œë“œ ê²€ì‚¬
                        is_match = False
                        if not keyword:
                            is_match = True
                        elif keyword.lower() in text.lower() or keyword.lower() in full_link.lower():
                            is_match = True
                        
                        if is_match:
                            if full_link not in links:
                                links.append(full_link)
                                match_count += 1
                                # ë¡œê·¸ì— ì°¾ì€ ê²ƒ í‘œì‹œ
                                st.write(f"âœ… ë°œê²¬: [{text}] -> {full_link}")
            
            if len(links) == 0:
                st.warning("âš ï¸ ê²Œì‹œê¸€ ë§í¬ íŒ¨í„´(wr_id)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²Œì‹œíŒ URLì´ ë§ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                st.success(f"ğŸ¯ ì´ {len(links)}ê°œì˜ ê²Œì‹œë¬¼ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
                
            return links

        except Exception as e:
            st.error(f"âŒ ì ‘ì† ì˜¤ë¥˜: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            
            title = soup.find(['h1', 'h2', 'h3', 'title'])
            # ëª¨ë°”ì¼ ê·¸ëˆ„ë³´ë“œ ë³¸ë¬¸ ID (bo_v_con)
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content") or soup.body
            
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-31'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open("backend/scripts/crawler.py", "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… ì´ˆì •ë°€ í¬ë¡¤ëŸ¬(í•œê¸€ íŒ¨ì¹˜ í¬í•¨) ì„¤ì¹˜ ì™„ë£Œ!")
