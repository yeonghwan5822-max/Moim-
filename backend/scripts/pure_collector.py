
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
import re
from datetime import datetime
import urllib3
from urllib.parse import urljoin

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class PureCollector:
    def __init__(self, phpsessid: str):
        self.session = requests.Session()
        self.session.cookies.set('PHPSESSID', phpsessid, domain='m.ebcblue.com')
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://m.ebcblue.com/',
        }
        self.school_keywords = ['ì¤‘ë„', 'í•™ì‹', 'ê³¼ì ', 'íŒ€í”Œ']
        self.raw_data = []

    def clean_text(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±° ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬"""
        text = re.sub(r'\s+', ' ', text).strip()
        # ë‚´ë¹„ê²Œì´ì…˜ ë…¸ì´ì¦ˆ ì œê±° (ì˜ˆì‹œ)
        noise_patterns = [r'ëª©ë¡', r'ìª½ì§€', r'ëŒ“ê¸€', r'ì´ì „ê¸€', r'ìƒë‹¨ìœ¼ë¡œ']
        for p in noise_patterns:
            text = text.replace(p, '')
        return text

    def smart_chunking(self, text: str, max_length=1000) -> list:
        """1000ì ì´ˆê³¼ ì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        if len(text) <= max_length:
            return [text]

        chunks = []
        current_chunk = ""
        # ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ ë’¤ì—ì„œ ë¶„í• 
        sentences = re.split(r'(?<=[.?\n])', text)
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) > max_length:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
            else:
                current_chunk += sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        return chunks

    def extract_tags(self, text: str) -> str:
        """ë³¸ë¬¸ì—ì„œ í•™êµ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        found = [kw for kw in self.school_keywords if kw in text]
        return ",".join(found) if found else ""

    def get_board_links(self, board_url: str, pages: int = 1) -> list:
        """ê²Œì‹œíŒì˜ ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ë§í¬ ìˆ˜ì§‘"""
        all_links = []
        base_url = board_url.split('?')[0] # Remove params for cleanliness if needed, but board.php needs params
        
        for page in range(1, pages + 1):
            target = f"{board_url}&page={page}"
            print(f"ğŸ“¡ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... ({target})")
            
            try:
                res = self.session.get(target, headers=self.headers, verify=False, timeout=10)
                res.encoding = res.apparent_encoding
                soup = BeautifulSoup(res.text, 'html.parser')
                
                found_on_page = 0
                for a in soup.find_all('a', href=True):
                    href = a['href']
                    if 'wr_id=' in href and 'bo_table=' in href:
                        if any(x in href for x in ['write', 'update', 'delete', 'search', 'login']): continue
                        
                        full_link = urljoin(board_url, href)
                        if full_link not in all_links:
                            all_links.append(full_link)
                            found_on_page += 1
                
                print(f"   -> {found_on_page}ê°œ ê¸€ ë°œê²¬")
                time.sleep(1) # ë¶€í•˜ ë°©ì§€
            except Exception as e:
                print(f"âŒ {page}í˜ì´ì§€ ì ‘ì† ì˜¤ë¥˜: {e}")
        
        return list(set(all_links)) # ìµœì¢… ì¤‘ë³µ ì œê±°

    def process_post(self, url: str, keyword: str = None):
        """ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ìˆ˜ì§‘ ë° ì €ì¥ (keyword í•„í„°ë§ ì¶”ê°€)"""
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            post_id = re.search(r'wr_id=(\d+)', url)
            post_id_val = post_id.group(1) if post_id else "0"
            
            # ì¹´í…Œê³ ë¦¬ (ê²Œì‹œíŒ ì´ë¦„) - íƒ€ì´í‹€ì´ë‚˜ ë„¤ë¹„ê²Œì´ì…˜ë°”ì—ì„œ ì¶”ì¶œ ì‹œë„
            title_node = soup.find(id="bo_v_title")
            if not title_node:
                title_node = soup.find(class_="bo_v_tit")
            title_text = title_node.get_text(strip=True) if title_node else ""

            # ë³´í†µ ì œëª© ìœ„ì— ì¹´í…Œê³ ë¦¬ê°€ ìˆê±°ë‚˜, URL bo_table íŒŒë¼ë¯¸í„° ì‚¬ìš©
            category_code = re.search(r'bo_table=([^&]+)', url)
            category = category_code.group(1) if category_code else "Unknown"

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_div = soup.find(id="bo_v_con") or soup.find(class_="view-content")
            if not content_div:
                print(f"âš ï¸ Error: ID {post_id_val} ë³¸ë¬¸ ì—†ìŒ - ê±´ë„ˆëœ€")
                return

            # HTML Cleaning
            clean_content = self.clean_text(content_div.get_text("\n", strip=True))
            
            # [Logic Update] Keyword Filtering
            if keyword:
                if (keyword not in title_text) and (keyword not in clean_content):
                    # í‚¤ì›Œë“œê°€ ì œëª©ì´ë‚˜ ë³¸ë¬¸ì— ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
                    return
            
            # Smart Chunking
            chunks = self.smart_chunking(clean_content)
            tags = self.extract_tags(clean_content)
            date_str = datetime.now().strftime("%Y-%m-%d") # ì‹¤ì œ ì‘ì„±ì¼ì„ íŒŒì‹±í•˜ë ¤ë©´ ë³„ë„ ë¡œì§ í•„ìš” (í˜„ì¬ëŠ” ì˜¤ëŠ˜ ë‚ ì§œ)

            # ë°ì´í„° ì €ì¥ êµ¬ì¡° ìƒì„±
            for idx, chunk in enumerate(chunks, 1):
                self.raw_data.append({
                    "ID": f"{post_id_val}_{idx}", # Unique ID for chunk
                    "Date": date_str,
                    "Category": category,
                    "Original_Text": chunk,
                    "Tags": tags,
                    "Target_Lang": "",     # ë¹ˆì¹¸
                    "Translated_Text": ""  # ë¹ˆì¹¸
                })
            
            print(f"âœ… ID {post_id_val} ìˆ˜ì§‘ ì™„ë£Œ (ì²­í¬: {len(chunks)}ê°œ)")

        except Exception as e:
            print(f"âŒ Error: ID {url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - {e}")

    def save_csv(self):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.raw_data:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.raw_data)
        file_name = "moim_raw_data.csv"
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆë‹¤ë©´ ì´ì–´ì„œ ì €ì¥ (Append)
        if os.path.exists(file_name):
            df.to_csv(file_name, mode='a', header=False, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ {len(df)}ê°œ ë°ì´í„°ë¥¼ ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
        else:
            df.to_csv(file_name, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ìƒˆë¡œìš´ íŒŒì¼ {file_name}ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    print("--- ğŸ¤– Pure Community Data Collector ---")
    phpsessid_input = input("ğŸ”‘ PHPSESSID ì…ë ¥: ").strip()
    target_url = input("ğŸ”— ìˆ˜ì§‘í•  ê²Œì‹œíŒ URL (ì˜ˆ: ...bo_table=free): ").strip()
    pages_input = input("ğŸ“„ ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 1): ").strip()
    
    pages = int(pages_input) if pages_input.isdigit() else 1
    
    collector = PureCollector(phpsessid_input)
    
    print("\nğŸš€ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    target_links = collector.get_board_links(target_url, pages)
    
    print(f"\nğŸ” ì´ {len(target_links)}ê°œì˜ ê²Œì‹œê¸€ì„ ìƒì„¸ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    for link in target_links:
        collector.process_post(link)
        time.sleep(0.5)
        
    collector.save_csv()
    print("\nâœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
