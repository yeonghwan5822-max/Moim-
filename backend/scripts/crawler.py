import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin

class EbcCrawler:
    def __init__(self, headless=True):
        # ë¸Œë¼ìš°ì € ì—†ì´ ê°€ë²¼ìš´ 'Requests' ì‚¬ìš©
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
        }

    def _init_driver(self):
        pass # ë¸Œë¼ìš°ì € ì•ˆ ì“°ë‹ˆê¹Œ íŒ¨ìŠ¤

    def close(self):
        pass # ë‹«ì„ ë¸Œë¼ìš°ì € ì—†ìŒ

    def login(self):
        pass # í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— êµ¬í˜„

    # [í•µì‹¬] ì•„ê¹Œ ì—ëŸ¬ë‚¬ë˜ ê·¸ í•¨ìˆ˜! (ì´ë¦„ ì¶”ê°€ë¨)
    def get_categorized_links(self, url, keyword=None):
        print(f"ğŸš€ [Stealth] Searching: {url}")
        
        # 1. ë§í¬ ìˆ˜ì§‘
        raw_links = self.get_post_links(url, keyword)
        
        # 2. ë¶„ë¥˜ (ì¼ë‹¨ 'normal'ì— ë‹¤ ëª°ì•„ë„£ê¸° -> ì—ëŸ¬ ë°©ì§€)
        return {
            'notice': [],
            'normal': raw_links
        }

    # ì‹¤ì œ ë§í¬ ì°¾ëŠ” ë¡œì§
    def get_post_links(self, url, keyword=None):
        links = []
        try:
            response = self.session.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê·¸ëˆ„ë³´ë“œ ê²Œì‹œê¸€ íŒ¨í„´ (wr_idê°€ ìˆëŠ” ë§í¬ ì°¾ê¸°)
            for a in soup.find_all('a', href=True):
                href = a['href']
                if 'wr_id=' in href and 'bo_table=' in href:
                    # 'ê¸€ì“°ê¸°', 'ë‹µë³€', 'ì‚­ì œ' ê°™ì€ ê¸°ëŠ¥ ë§í¬ ì œì™¸
                    if 'write' in href or 'delete' in href or 'update' in href:
                        continue
                        
                    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    full_link = urljoin(url, href)
                    if full_link not in links:
                        links.append(full_link)
            
            print(f"âœ… Found {len(links)} posts.")
            return links

        except Exception as e:
            print(f"âŒ Error fetching links: {e}")
            return []

    # ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (í˜¹ì‹œ ëª°ë¼ ë¯¸ë¦¬ ì¶”ê°€)
    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers)
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.title.string if soup.title else "No Title"
            # ë³¸ë¬¸ ëŒ€ì¶© ê¸ì–´ì˜¤ê¸° (id="bo_v_con" ë“± ê·¸ëˆ„ë³´ë“œ í‘œì¤€)
            content_div = soup.find(id="bo_v_con")
            content = content_div.get_text(strip=True) if content_div else "Content not found"
            return {'title': title, 'content': content, 'date': '2024-01-01'}
        except:
            return {'title': "Error", 'content': "", 'date': ""}
