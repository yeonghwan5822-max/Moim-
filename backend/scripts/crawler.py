import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, quote

class EbcCrawler:
    def __init__(self, headless=True):
        self.session = requests.Session()
        # ì‹¤ì œ ìµœì‹  ì•„ì´í° ë¸Œë¼ìš°ì €ì¸ ê²ƒì²˜ëŸ¼ ë” ì •êµí•˜ê²Œ ìœ„ì¥í•©ë‹ˆë‹¤.
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://m.ebcblue.com/'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        # [í•µì‹¬ ë¡œì§] í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê²Œì‹œíŒì˜ 'ê²€ìƒ‰ URL'ë¡œ ì¦‰ì‹œ ìš°íšŒí•©ë‹ˆë‹¤.
        search_url = url
        if keyword:
            # ê·¸ëˆ„ë³´ë“œ í‘œì¤€ ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì ìš© (ì œëª©+ë‚´ìš© ê²€ìƒ‰)
            clean_keyword = quote(keyword.encode('utf-8'))
            if '?' in url:
                search_url = f"{url}&sfl=wr_subject||wr_content&stx={clean_keyword}"
            else:
                search_url = f"{url}?sfl=wr_subject||wr_content&stx={clean_keyword}"
        
        print(f"ğŸ¯ ê²€ìƒ‰ ìš°íšŒ ì ‘ì†: {search_url}")
        raw_links = self.get_post_links(search_url, keyword)
        return {'notice': [], 'normal': raw_links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            # ì‚¬ì´íŠ¸ ì ‘ì† (ì¿ í‚¤ ìƒì„±ì„ ìœ„í•´ ì„¸ì…˜ ìœ ì§€)
            res = self.session.get(url, headers=self.headers, timeout=15)
            res.raise_for_status()
            
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ëª¨ë“  <a> íƒœê·¸ì—ì„œ ê²Œì‹œê¸€ ë²ˆí˜¸(wr_id)ê°€ í¬í•¨ëœ ë§í¬ë§Œ ì •ë°€ ì¶”ì¶œ
            for a in soup.find_all('a', href=True):
                href = a['href']
                # ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ íŒ¨í„´ë§Œ ìˆ˜ì§‘
                if 'wr_id=' in href and 'bo_table=' in href:
                    # ì“°ê¸°, ê²€ìƒ‰, ê´€ë¦¬ì ê¸°ëŠ¥ ë“± ì œì™¸
                    if any(x in href for x in ['write', 'search', 'admin', 'update', 'delete']):
                        continue
                        
                    full_link = urljoin(url, href)
                    if full_link not in links:
                        links.append(full_link)
            
            print(f"âœ… ë°œê²¬ëœ ê²Œì‹œê¸€: {len(links)}ê°œ")
            return links

        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì—ëŸ¬: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            # ì œëª© ì¶”ì¶œ (ëª¨ë°”ì¼ ë²„ì „ ëŒ€ì‘)
            title = soup.find('h1') or soup.find('h2') or soup.find('title')
            # ë³¸ë¬¸ ì¶”ì¶œ (ê·¸ëˆ„ë³´ë“œ í‘œì¤€ ID)
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content")
            
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True)[:1000] if content else "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'date': '2026-01-30'
            }
        except:
            return {'title': "ì—ëŸ¬", 'content': "", 'date': ""}
