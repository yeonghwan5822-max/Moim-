import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin, quote
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        # [í•µì‹¬] ë´‡ ì°¨ë‹¨ íšŒí”¼ìš© í—¤ë”
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://m.ebcblue.com/',
            'Accept-Language': 'ko-KR,ko;q=0.9'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        # ê²€ìƒ‰ì–´ ìœ ë¬´ì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬
        if keyword:
            st.info(f"ğŸš€ [ê²€ìƒ‰ ëª¨ë“œ] í‚¤ì›Œë“œ '{keyword}'ë¡œ ì§ì ‘ ì ‘ê·¼í•©ë‹ˆë‹¤.")
            encoded_kw = quote(keyword.encode('utf-8'))
            sep = "&" if "?" in url else "?"
            # ê²€ìƒ‰ ì¿¼ë¦¬ ì§ì ‘ ì£¼ì…
            target_url = f"{url}{sep}sfl=wr_subject||wr_content&stx={encoded_kw}"
        else:
            st.info(f"ğŸ“¡ [ëª©ë¡ ëª¨ë“œ] ì „ì²´ ëª©ë¡ì„ ìŠ¤ìº”í•©ë‹ˆë‹¤.")
            target_url = url

        links = self.get_post_links(target_url, keyword)
        return {'notice': [], 'normal': links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            # 1. ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ íšë“)
            self.session.get("https://m.ebcblue.com/", headers=self.headers, verify=False, timeout=5)
            
            # 2. ì‹¤ì œ íƒ€ê²Ÿ í˜ì´ì§€ ì ‘ì†
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # [ì§„ë‹¨] í˜„ì¬ í˜ì´ì§€ ìƒí™© ì¶œë ¥
            all_a = soup.find_all('a', href=True)
            with st.expander(f"ğŸ•µï¸â€â™‚ï¸ í˜ì´ì§€ ì§„ë‹¨ (ë§í¬ {len(all_a)}ê°œ ë°œê²¬)", expanded=True):
                if not all_a:
                    st.warning("ë§í¬ê°€ 0ê°œì…ë‹ˆë‹¤. ë´‡ ì°¨ë‹¨ë˜ì—ˆê±°ë‚˜ ë¡œê·¸ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                for a in all_a[:3]:
                    st.text(f"ìƒ˜í”Œ: [{a.get_text(strip=True)}] -> {a['href']}")

            # 3. ë§í¬ ìˆ˜ì§‘
            for a in all_a:
                href = a['href']
                # wr_id íŒ¨í„´ í™•ì¸
                if 'wr_id=' in href:
                    if any(bad in href for bad in ['write', 'update', 'delete', 'search', 'login']): continue
                    
                    full_link = urljoin(url, href)
                    
                    # ì´ë¯¸ ê²€ìƒ‰ëœ í˜ì´ì§€ë¼ë©´(stx í¬í•¨) í‚¤ì›Œë“œ ê²€ì‚¬ ë¶ˆí•„ìš”
                    if "stx=" in url:
                        if full_link not in links: links.append(full_link)
                    # ì¼ë°˜ ëª©ë¡ì´ë¼ë©´ í…ìŠ¤íŠ¸ ê²€ì‚¬
                    elif not keyword or (keyword in a.get_text() or keyword in full_link):
                        if full_link not in links: links.append(full_link)
            
            if links:
                st.success(f"ğŸ¯ {len(links)}ê°œì˜ ê²Œì‹œê¸€ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤!")
            return links

        except Exception as e:
            st.error(f"âŒ ì ‘ì† ì˜¤ë¥˜: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content") or soup.body
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-31'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
