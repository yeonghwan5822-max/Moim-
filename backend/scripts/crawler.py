import requests
from bs4 import BeautifulSoup
import time

class EbcCrawler:
    def __init__(self, headless=True):
        # ë¸Œë¼ìš°ì € ëŒ€ì‹  'ì„¸ì…˜'ì„ ì‚¬ìš© (ë¡œê·¸ì¸ ìœ ì§€ ë“± ê°€ëŠ¥)
        self.session = requests.Session()
        # ì‚¬ëŒì¸ ì²™ ìœ„ì¥í•˜ëŠ” ê°€ë©´ (User-Agent)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def _init_driver(self):
        # ë¸Œë¼ìš°ì €ë¥¼ ì•ˆ ì“°ë‹ˆ ì´ˆê¸°í™”í•  ê²Œ ì—†ìŒ
        pass

    def close(self):
        pass

    def get_post_links(self, url, keyword=None):
        print(f"ğŸš€ Fetching URL (Stealth Mode): {url}")
        try:
            # 1. ì›¹í˜ì´ì§€ ìš”ì²­ (ë¸Œë¼ìš°ì € ì—†ì´ ì ‘ì†)
            response = self.session.get(url, headers=self.headers)
            response.raise_for_status() # ì—ëŸ¬ ì²´í¬
            
            # 2. HTML í•´ì„
            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.title.string if soup.title else 'No Title'
            print(f"âœ… ì ‘ì† ì„±ê³µ! í˜ì´ì§€ ì œëª©: {title}")
            
            # 3. ë§í¬ ì°¾ê¸° (ê·¸ëˆ„ë³´ë“œ íŒ¨í„´: wr_id)
            links = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                # ê²Œì‹œê¸€ ë§í¬ íŒ¨í„´ì´ ë³´ì´ë©´ ìˆ˜ì§‘
                if 'wr_id=' in href and 'bo_table=' in href:
                    full_link = href if href.startswith('http') else url + href
                    links.append(full_link)
            
            print(f"Found {len(links)} links.")
            return links

        except Exception as e:
            print(f"âŒ Error: {e}")
            return []
