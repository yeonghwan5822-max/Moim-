import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse

class EbcCrawler:
    def __init__(self, headless=True):
        self.session = requests.Session()
        # ë¸Œë¼ìš°ì €ì¸ ì²™ ìœ„ì¥ë ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        print(f"ğŸš€ [Target] {url}")
        raw_links = self.get_post_links(url, keyword)
        return {'notice': [], 'normal': raw_links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ëª¨ë“  <a> íƒœê·¸ë¥¼ ë‹¤ ë’¤ì§‘ë‹ˆë‹¤.
            for a in soup.find_all('a', href=True):
                href = a['href']
                # ê·¸ëˆ„ë³´ë“œ í•µì‹¬ íŒ¨í„´: wr_id ë˜ëŠ” bo_tableì´ ë“¤ì–´ê°„ ëª¨ë“  ë§í¬
                if 'wr_id=' in href or 'bo_table=' in href:
                    # ì“°ê¸°, ìˆ˜ì •, ê´€ë¦¬ì ê¸°ëŠ¥ ë“± ë¶ˆí•„ìš”í•œ ë§í¬ ì œì™¸
                    if any(x in href for x in ['write', 'update', 'delete', 'token', 'admin']):
                        continue
                    
                    full_link = urljoin(url, href)
                    
                    # í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´ ì œëª©ì´ë‚˜ ë§í¬ì— í¬í•¨ëœ ê²½ìš°ë§Œ ìˆ˜ì§‘
                    if keyword:
                        if keyword in a.get_text() or keyword in full_link:
                            if full_link not in links: links.append(full_link)
                    else:
                        if full_link not in links: links.append(full_link)
            
            # ë§Œì•½ ì•„ë¬´ê²ƒë„ ëª» ì°¾ì•˜ë‹¤ë©´? ë” ë„“ì€ ë²”ìœ„ë¡œ í•œ ë²ˆ ë” ì‹œë„
            if not links:
                print("âš ï¸ No standard patterns found. Trying broad search...")
                for a in soup.find_all('a', href=True):
                    if '/bbs/board.php' in a['href'] and 'wr_id' in a['href']:
                        full_link = urljoin(url, a['href'])
                        if full_link not in links: links.append(full_link)

            print(f"âœ… Found {len(links)} candidate links.")
            return links

        except Exception as e:
            print(f"âŒ Fetch Error: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers)
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find('h1') or soup.find('h2') or soup.title
            title_text = title.get_text(strip=True) if title else "No Title"
            
            # ë³¸ë¬¸ ì˜ì—­ íƒìƒ‰ (GnuBoard í‘œì¤€ IDë“¤)
            content_div = soup.find(id="bo_v_con") or soup.find(class_="view-content") or soup.body
            content_text = content_div.get_text(strip=True)[:500] if content_div else "No Content"
            
            return {'title': title_text, 'content': content_text, 'date': '2026-01-30'}
        except:
            return {'title': "Error", 'content': "", 'date': ""}
