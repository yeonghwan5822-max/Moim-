import requests
from bs4 import BeautifulSoup
import time
import streamlit as st
from urllib.parse import urljoin

class EbcCrawler:
    def __init__(self, headless=True):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        # [ì§„ë‹¨] ì•± í™”ë©´ì— í˜„ì¬ ìƒíƒœë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        st.write(f"ğŸ” **ì ‘ì† ì‹œë„:** {url}")
        raw_links = self.get_post_links(url, keyword)
        return {'notice': [], 'normal': raw_links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, timeout=10)
            
            # [ì§„ë‹¨] HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
            if res.status_code == 200:
                st.success(f"âœ… ì„œë²„ ì‘ë‹µ ì„±ê³µ (200)")
            else:
                st.error(f"âŒ ì„œë²„ ì‘ë‹µ ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ: {res.status_code})")
                return []

            soup = BeautifulSoup(res.text, 'html.parser')
            
            # [ì§„ë‹¨] í˜ì´ì§€ ì œëª© í™•ì¸
            title = soup.title.string if soup.title else "ì œëª© ì—†ìŒ"
            st.write(f"ğŸ“„ **í˜ì´ì§€ ì œëª©:** {title}")

            # ëª¨ë“  <a> íƒœê·¸ íƒìƒ‰ (ë” ê³µê²©ì ì¸ ì¶”ì¶œ)
            all_a = soup.find_all('a', href=True)
            st.write(f"ğŸ”— **í˜ì´ì§€ ë‚´ ì´ ë§í¬ ìˆ˜:** {len(all_a)}ê°œ")

            for a in all_a:
                href = a['href']
                text = a.get_text(strip=True)
                
                # ê²Œì‹œê¸€ë¡œ ì¶”ì •ë˜ëŠ” ëª¨ë“  íŒ¨í„´ ìˆ˜ì§‘
                is_post = any(p in href for p in ['wr_id=', 'bo_table=', 'board.php'])
                is_junk = any(j in href for j in ['write', 'update', 'delete', 'token', 'search'])
                
                if is_post and not is_junk:
                    full_link = urljoin(url, href)
                    if not keyword or (keyword in text or keyword in full_link):
                        if full_link not in links:
                            links.append(full_link)
            
            st.write(f"ğŸ¯ **ìµœì¢… ì¶”ì¶œëœ ê²Œì‹œê¸€:** {len(links)}ê°œ")
            return links

        except Exception as e:
            st.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return []

    def get_post_content(self, url):
        return {'title': 'Test', 'content': 'Test Content', 'date': '2026-01-30'}
