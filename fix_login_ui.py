import os

target_file = "backend/streamlit_app.py"

# 1. ë¡œê·¸ì¸ ê²€ì¦ ê°•í™” + 2. í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ í…Œì´ë¸” ê¸°ëŠ¥ ì¶”ê°€
final_code = """import streamlit as st
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self):
        self.session = requests.Session()
        # ëª¨ë°”ì¼ ë¸Œë¼ìš°ì €ì¸ ì²™ ìœ„ì¥ (ì¤‘ìš”)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Mobile Safari/537.36',
            'Referer': 'https://m.ebcblue.com/',
            'Origin': 'https://m.ebcblue.com'
        }
        self.login_url = "https://m.ebcblue.com/bbs/login_check.php"

    def login(self, user_id, user_pw):
        try:
            # 1. ë¡œê·¸ì¸ ì‹œë„
            data = {'mb_id': user_id, 'mb_password': user_pw, 'url': 'https://m.ebcblue.com/'}
            self.session.post(self.login_url, data=data, headers=self.headers, verify=False)
            
            # 2. [í•µì‹¬] ì§„ì§œ ë¡œê·¸ì¸ì´ ëëŠ”ì§€ ë©”ì¸ í˜ì´ì§€ ê°€ì„œ í™•ì¸
            res = self.session.get('https://m.ebcblue.com/', headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            
            # 'ë¡œê·¸ì•„ì›ƒ' ë²„íŠ¼ì´ ìˆì–´ì•¼ ì§„ì§œ ë¡œê·¸ì¸ ëœ ê²ƒì„
            if 'ë¡œê·¸ì•„ì›ƒ' in res.text or 'logout' in res.text:
                return True
            else:
                return False
        except: return False

    def scan_links(self, url, keyword):
        st.info("ğŸ” í˜ì´ì§€ë¥¼ ìŠ¤ìº” ì¤‘ì…ë‹ˆë‹¤...")
        found_data = []
        
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')

            # 1. HTML ì „ì²´ì—ì„œ í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if keyword and keyword not in res.text:
                st.error(f"âŒ í˜ì´ì§€ ì†ŒìŠ¤ ì½”ë“œ ë‚´ì— '{keyword}'ë¼ëŠ” ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                st.warning("ë¡œê·¸ì¸ì´ ë˜ì—ˆìŒì—ë„ ì•ˆ ë³´ì¸ë‹¤ë©´, í•´ë‹¹ ê²Œì‹œë¬¼ì´ 1í˜ì´ì§€ì— ì—†ê±°ë‚˜ ê¶Œí•œì´ ì—†ëŠ” ê²Œì‹œíŒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                return []

            # 2. ë§í¬ ìˆ˜ì§‘ ë° ì •ë¦¬
            for a in soup.find_all('a', href=True):
                text = a.get_text(strip=True)
                href = a['href']
                full_link = urljoin(url, href)

                # í‚¤ì›Œë“œê°€ ìˆê±°ë‚˜, ì—†ìœ¼ë©´ ëª¨ë‘ ìˆ˜ì§‘
                if not keyword or (keyword and keyword in text):
                    # ìë°”ìŠ¤í¬ë¦½íŠ¸ë‚˜ ë¹ˆ ë§í¬ ì œì™¸
                    if len(text) > 0 and 'javascript' not in href:
                        found_data.append({
                            "ì œëª©": text,
                            "ë§í¬": full_link
                        })
            
            return found_data

        except Exception as e:
            st.error(f"ì—ëŸ¬: {e}")
            return []

# ==========================================
# UI êµ¬ì„±
# ==========================================
st.set_page_config(page_title="MOIM ë²ˆì—­ê¸° (Final)", layout="wide")
st.title("ğŸ” MOIM ë²ˆì—­ê¸° : íšŒì› ì „ìš© ê²€ìƒ‰")

with st.sidebar:
    st.header("ë¡œê·¸ì¸")
    user_id = st.text_input("ì•„ì´ë””", key="uid")
    user_pw = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="upw")

st.subheader("1. ê²€ìƒ‰ ì„¤ì •")
url = st.text_input("íƒ€ê²Ÿ URL", "http://m.ebcblue.com/")
keyword = st.text_input("ì°¾ì„ í‚¤ì›Œë“œ (ì˜ˆ: ì‹¤ìŠµ)", placeholder="ë¹„ì›Œë‘ë©´ ëª¨ë“  ë§í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤")

if st.button("ğŸš€ ë¡œê·¸ì¸í•˜ê³  ê²€ìƒ‰ ì‹œì‘"):
    if not user_id or not user_pw:
        st.error("ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        crawler = EbcCrawler()
        status = st.empty()
        status.info("ğŸ”‘ ë¡œê·¸ì¸ ê²€ì¦ ì¤‘...")
        
        # ì§„ì§œ ë¡œê·¸ì¸ì´ ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if crawler.login(user_id, user_pw):
            status.success("âœ… ë¡œê·¸ì¸ í™•ì¸ ì™„ë£Œ! (ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ ê°ì§€ë¨)")
            
            results = crawler.scan_links(url, keyword)
            
            if results:
                st.success(f"ğŸ‰ '{keyword}' ê´€ë ¨ í•­ëª© {len(results)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                
                # [ê°œì„ ëœ UI] í´ë¦­ ê°€ëŠ¥í•œ ë°ì´í„°í”„ë ˆì„
                df = pd.DataFrame(results)
                
                # ë§í¬ë¥¼ í´ë¦­ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                st.write("### ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ (ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”)")
                
                # ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥ (Streamlitì—ì„œ ë§í¬ í´ë¦­ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
                md_table = "| ì œëª© | ë°”ë¡œê°€ê¸° |\n|---|---|\n"
                for row in results:
                    md_table += f"| {row['ì œëª©']} | [ì´ë™í•˜ê¸°]({row['ë§í¬']}) |\n"
                
                st.markdown(md_table)
                
            else:
                st.warning("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            status.error("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨! (ì•„ì´ë””/ë¹„ë²ˆì´ í‹€ë ¸ê±°ë‚˜, ì„œë²„ê°€ ë¡œê·¸ì¸ì„ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤)")
            st.caption("íŒ: 'ë¡œê·¸ì¸' ë²„íŠ¼ì´ ì—¬ì „íˆ ë³´ì´ë©´ ì‹¤íŒ¨í•œ ê²ƒì…ë‹ˆë‹¤.")
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(final_code)

print("âœ… [íŒ¨ì¹˜ ì™„ë£Œ] ë¡œê·¸ì¸ ê²€ì¦ ê°•í™” + ê²°ê³¼ í…Œì´ë¸” ë””ìì¸ ê°œì„ ")
