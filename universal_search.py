import os

# 1. ê²½ë¡œ ë³´ì¥
os.makedirs("backend/scripts", exist_ok=True)

# 2. ë²”ìš© í¬ë¡¤ëŸ¬ ì—”ì§„ (ì–´ë–¤ í‚¤ì›Œë“œë“  ëŒ€ì‘ ê°€ëŠ¥)
crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin
import urllib3

# ë³´ì•ˆ ì¸ì¦ì„œ ì˜¤ë¥˜ ë¬´ì‹œ ì„¤ì •
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
            'Referer': 'https://m.ebcblue.com/'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        \"\"\"ì‚¬ìš©ìê°€ ì…ë ¥í•œ keywordë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"
        search_msg = f"'{keyword}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì¤‘..." if keyword else "ì „ì²´ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘..."
        st.info(f"ğŸŒ {search_msg} | ëŒ€ìƒ: {url}")
        
        raw_links = self.get_post_links(url, keyword)
        return {'notice': [], 'normal': raw_links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            # SSL ìš°íšŒ(verify=False) ë° ì ‘ì†
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            res.raise_for_status()
            soup = BeautifulSoup(res.content, 'html.parser')
            
            # í˜ì´ì§€ ë‚´ ëª¨ë“  <a> íƒœê·¸ë¥¼ ë’¤ì ¸ì„œ ê²Œì‹œê¸€ íŒ¨í„´ ì¶”ì¶œ
            for a in soup.find_all('a', href=True):
                href = a['href']
                text = a.get_text(strip=True)
                
                # ê·¸ëˆ„ë³´ë“œ ê²Œì‹œê¸€ í‘œì¤€ íŒ¨í„´(wr_id)
                if 'wr_id=' in href and 'bo_table=' in href:
                    if any(x in href for x in ['write', 'update', 'delete', 'search']): 
                        continue
                    
                    full_link = urljoin(url, href)
                    
                    # [í•µì‹¬] ë²”ìš© í‚¤ì›Œë“œ ë§¤ì¹­ ë¡œì§
                    # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì „ë¶€ ìˆ˜ì§‘, ìˆìœ¼ë©´ ì œëª©ì´ë‚˜ ë§í¬ì— í¬í•¨ëœ ê²ƒë§Œ ìˆ˜ì§‘
                    if not keyword:
                        if full_link not in links: links.append(full_link)
                    else:
                        # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë§¤ì¹­ (ì˜ì–´ í‚¤ì›Œë“œ ëŒ€ë¹„)
                        if keyword.lower() in text.lower() or keyword.lower() in full_link.lower():
                            if full_link not in links: links.append(full_link)
            
            if links:
                st.success(f"ğŸ¯ ê²€ìƒ‰ ê²°ê³¼: {len(links)}ê°œì˜ ê²Œì‹œë¬¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            else:
                st.warning("âš ï¸ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
            return list(set(links))
        except Exception as e:
            st.error(f"âŒ ì ‘ì† ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def get_post_content(self, url):
        \"\"\"ê²Œì‹œê¸€ ë³¸ë¬¸ì„ ê¸ì–´ì˜¤ëŠ” ê¸°ëŠ¥\"\"\"
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            soup = BeautifulSoup(res.content, 'html.parser')
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content")
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-31'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open("backend/scripts/crawler.py", "w", encoding="utf-8") as f:
    f.write(crawler_code)

# 3. í™˜ê²½ íŒŒì¼ ìµœì‹ í™”
with open("backend/requirements.txt", "w", encoding="utf-8") as f:
    f.write("streamlit\\nrequests\\nbeautifulsoup4\\ndeepl\\npython-dotenv\\nchromadb\\nsoynlp\\nurllib3")

print("ğŸ’ [Anti-Gravity] ë²”ìš© ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ ì™„ë£Œ!")
