import os

target_file = "backend/streamlit_app.py"

# HTML ì†ŒìŠ¤ ìì²´ë¥¼ ë’¤ì ¸ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ëŠ” ì´ˆì •ë°€ ì§„ë‹¨ ì½”ë“œ
debug_code = """import streamlit as st
import requests
from bs4 import BeautifulSoup
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://m.ebcblue.com/'
        }
        self.login_url = "https://m.ebcblue.com/bbs/login_check.php"

    def login(self, user_id, user_pw):
        try:
            data = {'mb_id': user_id, 'mb_password': user_pw, 'url': 'https://m.ebcblue.com/'}
            res = self.session.post(self.login_url, data=data, headers=self.headers, verify=False)
            if "ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë¦½ë‹ˆë‹¤" in res.text or "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”" in res.text:
                return False
            return True
        except: return False

    def deep_scan(self, url, keyword):
        st.info("ğŸ”¬ í˜ì´ì§€ë¥¼ í•´ë¶€í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
        
        try:
            res = self.session.get(url, headers=self.headers, verify=False)
            res.encoding = res.apparent_encoding
            html_content = res.text
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # 1. HTML ì†ŒìŠ¤ ì „ì²´ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
            st.write(f"### 1. ì†ŒìŠ¤ì½”ë“œ ì •ë°€ ê²€ìƒ‰ ê²°ê³¼ (í‚¤ì›Œë“œ: '{keyword}')")
            if keyword and keyword in html_content:
                st.success(f"âœ… HTML ì†ŒìŠ¤ì½”ë“œ ì•ˆì—ì„œ '{keyword}'ë¼ëŠ” ë‹¨ì–´ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                st.text("ì•„ë˜ 'ë°œê²¬ëœ ìœ„ì¹˜'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                
                # í‚¤ì›Œë“œ ì£¼ë³€ 100ê¸€ì ë³´ì—¬ì£¼ê¸°
                idx = html_content.find(keyword)
                start = max(0, idx - 100)
                end = min(len(html_content), idx + 100)
                snippet = html_content[start:end]
                st.code(snippet, language='html')
            elif keyword:
                st.error(f"âŒ HTML ì†ŒìŠ¤ì½”ë“œ ì „ì²´ë¥¼ ë’¤ì¡Œì§€ë§Œ '{keyword}'ë¼ëŠ” ë‹¨ì–´ê°€ ì•„ì˜ˆ ì—†ìŠµë‹ˆë‹¤.")
                st.warning("ì˜¤íƒ€ê°€ ìˆê±°ë‚˜, ë¡œê·¸ì¸ì´ í’€ë ¤ì„œ ë‹¤ë¥¸ í™”ë©´ì´ ë³´ì´ëŠ” ê²ƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # 2. ëª¨ë“  ë§í¬(a íƒœê·¸) ì „ìˆ˜ ì¡°ì‚¬
            st.divider()
            st.write("### 2. í˜ì´ì§€ ë‚´ ëª¨ë“  ë§í¬ ëª©ë¡ (í•„í„° ì—†ìŒ)")
            
            links = soup.find_all('a', href=True)
            found_data = []
            
            for i, a in enumerate(links):
                text = a.get_text(strip=True)
                href = a['href']
                
                # í‚¤ì›Œë“œê°€ ìˆê±°ë‚˜, í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ë‹¤ ë„£ê¸°
                if not keyword or (keyword and keyword in text):
                    found_data.append({"No": i+1, "í…ìŠ¤íŠ¸": text, "ì£¼ì†Œ(href)": href})

            if found_data:
                st.success(f"ğŸ” ì´ {len(found_data)}ê°œì˜ ë§í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                st.table(found_data)
            else:
                st.warning("ë§í¬ ëª©ë¡ì—ì„œë„ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            st.error(f"ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

# UI êµ¬ì„±
st.set_page_config(page_title="MOIM ì´ˆì •ë°€ ì§„ë‹¨", layout="wide")
st.title("ğŸ”¬ MOIM ë²ˆì—­ê¸° : ì´ˆì •ë°€ ë‚´ì‹œê²½ ëª¨ë“œ")

with st.sidebar:
    st.header("ë¡œê·¸ì¸")
    user_id = st.text_input("ì•„ì´ë””", key="uid")
    user_pw = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="upw")

url = st.text_input("íƒ€ê²Ÿ URL", "http://m.ebcblue.com/")
keyword = st.text_input("í™•ì¸í•  í‚¤ì›Œë“œ (ì˜ˆ: ì‹¤ìŠµ)", "ì‹¤ìŠµ")

if st.button("ğŸš€ ë¡œê·¸ì¸í•˜ê³  í•´ë¶€ ì‹œì‘"):
    if not user_id or not user_pw:
        st.error("ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        crawler = EbcCrawler()
        if crawler.login(user_id, user_pw):
            st.success("ë¡œê·¸ì¸ ì„±ê³µ! ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            crawler.deep_scan(url, keyword)
        else:
            st.error("ë¡œê·¸ì¸ ì‹¤íŒ¨")
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(debug_code)

print("âœ… [ì´ˆì •ë°€ ì§„ë‹¨ íŒ¨ì¹˜] ì™„ë£Œ. ì´ì œ ì†ŒìŠ¤ì½”ë“œê¹Œì§€ ë’¤ì§‘ë‹ˆë‹¤.")

