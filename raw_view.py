import os

# í¬ë¡¤ëŸ¬ ì½”ë“œ (crawler.py) - ë…¸í•„í„°(No-Filter) ëª¨ë“œ
crawler_code = """import requests
from bs4 import BeautifulSoup
import streamlit as st
from urllib.parse import urljoin
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, **kwargs):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        st.info(f"ğŸ” [ë…¸í•„í„° ëª¨ë“œ] ì ‘ì† URL: {url}")
        links = self.get_post_links(url, keyword)
        return {'notice': [], 'normal': links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # í˜ì´ì§€ì— ìˆëŠ” ëª¨ë“  <a> íƒœê·¸ ìˆ˜ì§‘
            all_a = soup.find_all('a', href=True)
            
            # [ì§„ë‹¨] ì‹¤ì œ ë§í¬ ëª¨ì–‘ì„ í™”ë©´ì— ì°ì–´ë´…ë‹ˆë‹¤ (ìƒìœ„ 10ê°œ)
            with st.expander("ğŸ‘€ í¬ë¡¤ëŸ¬ê°€ ë³´ê³  ìˆëŠ” ì‹¤ì œ ë§í¬ë“¤ (ì—¬ê¸°ë¥¼ ëˆŒëŸ¬ í™•ì¸)", expanded=True):
                st.write(f"ì´ ë°œê²¬ëœ ë§í¬ ìˆ˜: {len(all_a)}ê°œ")
                sample_links = []
                for a in all_a[:10]:
                    sample_links.append(f"[{a.get_text(strip=True)}] -> {a['href']}")
                st.code("\\n".join(sample_links))

            # [ìˆ˜ì§‘ ë¡œì§ ëŒ€í­ ì™„í™”]
            for a in all_a:
                href = a['href']
                text = a.get_text(strip=True)
                full_link = urljoin(url, href)
                
                # ì¡°ê±´: ê·¸ëƒ¥ 'ê¸€ì“°ê¸°(write)', 'ê²€ìƒ‰(search)' ê°™ì€ ê²Œ ì•„ë‹ˆë©´ ë‹¤ ê°€ì ¸ì˜µë‹ˆë‹¤.
                # wr_idë‚˜ bo_table ê²€ì‚¬ë¥¼ ëºìŠµë‹ˆë‹¤.
                is_junk = any(x in href for x in ['write', 'update', 'delete', 'search', 'login', 'logout', 'password'])
                
                if not is_junk:
                    # ë§í¬ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´(ë©”ì¸í™”ë©´ ì´ë™ ë“±) ì œì™¸
                    if len(href) > 3:
                         # í‚¤ì›Œë“œ í•„í„°ë§ (í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê²€ì‚¬)
                        if keyword:
                            if keyword.lower() in text.lower() or keyword.lower() in full_link.lower():
                                if full_link not in links: links.append(full_link)
                        else:
                            # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ ìˆ˜ì§‘
                            if full_link not in links: links.append(full_link)
            
            if links:
                st.success(f"ğŸ¯ í•„í„° í•´ì œ í›„ {len(links)}ê°œì˜ ë§í¬ë¥¼ ê±´ì¡ŒìŠµë‹ˆë‹¤!")
            else:
                st.error("âŒ í•„í„°ë¥¼ ë‹¤ ê»ëŠ”ë°ë„ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ 'ì‹¤ì œ ë§í¬ë“¤'ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                
            return links

        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content") or soup.body
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-31'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open("backend/scripts/crawler.py", "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… ë…¸í•„í„°(No-Filter) ì§„ë‹¨ ëª¨ë“œ ì„¤ì¹˜ ì™„ë£Œ!")
