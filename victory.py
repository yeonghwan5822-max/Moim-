import os

target_file = "backend/scripts/crawler.py"

crawler_code = """import requests
from bs4 import BeautifulSoup
import time
import streamlit as st
from urllib.parse import urljoin, quote
import urllib3

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EbcCrawler:
    def __init__(self, headless=True):
        self.session = requests.Session()
        # [í•µì‹¬ 1] ì‹¤ì œ ìµœì‹  ë¸Œë¼ìš°ì €ì™€ ì™„ë²½í•˜ê²Œ ë˜‘ê°™ì€ ì‹ ë¶„ì¦(Header) ìœ„ì¡°
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://m.ebcblue.com/',
            'Connection': 'keep-alive'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        # [í•µì‹¬ 2] í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ëª©ë¡ì„ ë’¤ì§€ëŠ” ëŒ€ì‹ , ê²€ìƒ‰ ëª…ë ¹ì–´ë¥¼ ì§ì ‘ ì£¼ì…í•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ 'ëª©ë¡ ë³´ê¸°'ê°€ ë§‰í˜€ìˆì–´ë„ ë°ì´í„°ë¥¼ ëš«ê³  ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        final_url = url
        if keyword:
            encoded_kw = quote(keyword.encode('utf-8'))
            sep = "&" if "?" in url else "?"
            # ê·¸ëˆ„ë³´ë“œ í‘œì¤€ ê²€ìƒ‰ ì¿¼ë¦¬ ì£¼ì… (ì œëª©+ë‚´ìš© ê²€ìƒ‰)
            final_url = f"{url}{sep}sfl=wr_subject||wr_content&stx={encoded_kw}"
            st.info(f"ğŸš€ [ì§ì ‘ íƒ€ê²© ëª¨ë“œ] ê²€ìƒ‰ì–´ë¡œ ë°”ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤: {keyword}")
        else:
            st.info(f"ğŸ“¡ [ì¼ë°˜ ëª¨ë“œ] ì „ì²´ ëª©ë¡ì„ ìŠ¤ìº”í•©ë‹ˆë‹¤.")
            
        raw_links = self.get_post_links(final_url, keyword)
        return {'notice': [], 'normal': raw_links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            # [í•µì‹¬ 3] ì¿ í‚¤ ì„¸ì…˜ ì‘ì—… (ë¬¸ ì—´ê¸°)
            # ë©”ì¸ í˜ì´ì§€ë¥¼ ë¨¼ì € ë°©ë¬¸í•´ì„œ ì¿ í‚¤ë¥¼ ì–»ìŠµë‹ˆë‹¤.
            self.session.get("https://m.ebcblue.com/", headers=self.headers, verify=False, timeout=10)
            
            # ì‹¤ì œ íƒ€ê²Ÿ í˜ì´ì§€ ì ‘ì†
            res = self.session.get(url, headers=self.headers, verify=False, timeout=15)
            # í•œê¸€ ê¹¨ì§ ë°©ì§€
            res.encoding = res.apparent_encoding
            
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ì§„ë‹¨ ë¡œê·¸ ì¶œë ¥
            all_a = soup.find_all('a', href=True)
            with st.expander(f"ğŸ•µï¸â€â™‚ï¸ ì§„ë‹¨ ê²°ê³¼ (ì´ ë§í¬ {len(all_a)}ê°œ)", expanded=True):
                # ìƒìœ„ 3ê°œ ë§í¬ë§Œ ë³´ì—¬ì¤˜ì„œ í˜„ì¬ ì–´ë””ì— ìˆëŠ”ì§€ í™•ì¸
                for a in all_a[:3]:
                    st.text(f"[{a.get_text(strip=True)}] -> {a['href']}")

            for a in all_a:
                href = a['href']
                text = a.get_text(strip=True)
                
                # wr_idê°€ ìˆëŠ” ì§„ì§œ ê²Œì‹œê¸€ë§Œ ê³¨ë¼ëƒ„
                if 'wr_id=' in href:
                    # ì“°ê¸°, ì‚­ì œ ë“± ë¶ˆí•„ìš”í•œ ê¸°ëŠ¥ ì œì™¸
                    if any(x in href for x in ['write', 'update', 'delete', 'search']):
                        continue
                        
                    full_link = urljoin(url, href)
                    
                    if full_link not in links:
                        # ì´ë¯¸ ê²€ìƒ‰ì–´ë¡œ í•„í„°ë§ëœ í˜ì´ì§€ë¼ë©´(URLì— stxê°€ ìˆë‹¤ë©´) ê·¸ëƒ¥ ë‹¤ ë‹´ìŠµë‹ˆë‹¤.
                        if "stx=" in url:
                            links.append(full_link)
                        # ê·¸ê²Œ ì•„ë‹ˆë¼ë©´ í…ìŠ¤íŠ¸ ê²€ì‚¬
                        elif not keyword or (keyword in text or keyword in full_link):
                            links.append(full_link)
            
            if links:
                st.success(f"ğŸ¯ {len(links)}ê°œì˜ ê²Œì‹œê¸€ì„ ì„±ê³µì ìœ¼ë¡œ í™•ë³´í–ˆìŠµë‹ˆë‹¤!")
            else:
                st.warning("âš ï¸ ì—¬ì „íˆ ê²Œì‹œê¸€ì´ ì•ˆ ë³´ì¸ë‹¤ë©´, ì‚¬ì´íŠ¸ê°€ ë¡œê·¸ì¸ì„ ìš”êµ¬í•˜ëŠ” ê²ƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            return links
        except Exception as e:
            st.error(f"âŒ ì ‘ì† ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers, verify=False, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find(['h1', 'h2', 'title'])
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content") or soup.body
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True) if content else "ë³¸ë¬¸ ì—†ìŒ",
                'date': '2026-01-31'
            }
        except: return {'title': 'Error', 'content': '', 'date': ''}
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… ìµœì¢… ìŠ¹ë¦¬ íŒ¨ì¹˜(ì„¸ì…˜+í—¤ë”+ê²€ìƒ‰ì£¼ì…)ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")import os

target_file = "backend/scripts/crawler.py"

crawler_code = """import requests
from bs4 import BeautifulSoup
import time
import streamlit as st
from urllib.parse import urljoin, quote

class EbcCrawler:
    def __init__(self, headless=True):
        self.session = requests.Session()
        # ì‹¤ì œ ìµœì‹  ë¸Œë¼ìš°ì €ì™€ ë˜‘ê°™ì€ í™˜ê²½ ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://m.ebcblue.com/',
            'Connection': 'keep-alive'
        }

    def _init_driver(self): pass
    def close(self): pass

    def get_categorized_links(self, url, keyword=None, *args, **kwargs):
        # [í•µì‹¬] í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì§ì ‘ ì‚¬ìš©í•˜ë„ë¡ URL ë³€í™˜
        final_url = url
        if keyword:
            encoded_kw = quote(keyword.encode('utf-8'))
            sep = "&" if "?" in url else "?"
            final_url = f"{url}{sep}sfl=wr_subject||wr_content&stx={encoded_kw}"
        
        st.write(f"ğŸ” **ì ‘ì† URL:** {final_url}")
        raw_links = self.get_post_links(final_url, keyword)
        return {'notice': [], 'normal': raw_links}

    def get_post_links(self, url, keyword=None):
        links = []
        try:
            # ì²« ë²ˆì§¸ ì ‘ì†ìœ¼ë¡œ ì¿ í‚¤ íšë“
            self.session.get("https://m.ebcblue.com/", headers=self.headers, timeout=10)
            res = self.session.get(url, headers=self.headers, timeout=15)
            res.raise_for_status()
            
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ëª¨ë“  <a> íƒœê·¸ë¥¼ ë’¤ì ¸ì„œ ê²Œì‹œê¸€ ë²ˆí˜¸(wr_id)ê°€ ìˆëŠ” ê²ƒë§Œ ì¶”ì¶œ
            all_a = soup.find_all('a', href=True)
            for a in all_a:
                href = a['href']
                text = a.get_text(strip=True)
                
                # ê·¸ëˆ„ë³´ë“œ ì „ìš© ê²Œì‹œê¸€ ë§í¬ íŒ¨í„´
                if 'wr_id=' in href and 'bo_table=' in href:
                    # ì“°ê¸°, ì‚­ì œ ë“± ë¶ˆí•„ìš”í•œ ê¸°ëŠ¥ ì œì™¸
                    if any(x in href for x in ['write', 'update', 'delete', 'search']):
                        continue
                        
                    full_link = urljoin(url, href)
                    # ì¤‘ë³µ ì œê±° ë° í‚¤ì›Œë“œ ê²€ì¦
                    if full_link not in links:
                        if not keyword or (keyword in text or keyword in full_link):
                            links.append(full_link)
            
            st.write(f"ğŸ¯ **ë°œê²¬ëœ ì‹¤ìŠµ ê²Œì‹œê¸€:** {len(links)}ê°œ")
            return links
        except Exception as e:
            st.error(f"âŒ ì ‘ì† ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def get_post_content(self, url):
        try:
            res = self.session.get(url, headers=self.headers)
            soup = BeautifulSoup(res.text, 'html.parser')
            title = soup.find('h1') or soup.find('h2') or soup.find('title')
            content = soup.find(id="bo_v_con") or soup.find(class_="view-content")
            return {
                'title': title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                'content': content.get_text(strip=True)[:1000] if content else "ë‚´ìš© ì—†ìŒ",
                'date': '2026-01-30'
            }
        except: return {'title': "Error", 'content': "", 'date': ""}
"""

with open(target_file, "w", encoding="utf-8") as f:
    f.write(crawler_code)

print("âœ… ìµœì¢… ìŠ¹ë¦¬ íŒ¨ì¹˜ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
